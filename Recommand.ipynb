{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **추천시스템**\n",
    "## **추천시스템 정리**\n",
    "1. **내용 기반 추천시스템 : 객관적인 내용을** 기준으로 모델링\n",
    "1. **협업필터링** : 영화/ 사용자별 선호도 정보를 활용하여 모델링\n",
    "    1. **사용자/상품기반 협업필터링 : 동질의 사용자/ 상품정보를** 활용한 학습\n",
    "    1. **잠재성요인 모델 협업필터링 : 전체 테이블로** 학습하여 $U, V$ 근사행렬 압축한 뒤, Broad Cast로 예측모델을 생성합니다\n",
    "- 주요 추천 알고리즘 목록\n",
    "<div><img src=\"./data/collab.png\" width=500/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "# **1 문서 분석 알고리즘**\n",
    "[**(GitHub)**](https://github.com/your-first-ml-book/Examples) **처음 배우는 머신러닝** 11장 문서 분석 시스템 만들기\n",
    "```\n",
    "ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
    "spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question\n",
    "```\n",
    "## **01 스팸 구분학습 모델링**\n",
    "- Logistic Regression 학습모델을 사용합니다\n",
    "\n",
    "### **1) 스팸 데이터 불러오기**\n",
    "- 스팸/ 노스팸을 기준으로 **label을** 추가합니다\n",
    "- **8,713 개의 token** 으로 구성된 학습목록을 생성합니다\n",
    "- 활용시 **vocabluary** 목록과 **tf-idf** 로 변환 뒤 예측 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5574, 8713)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정상 메일이면 0, 스펨이면 1 을 Label 합니다\n",
    "# vocabulary : 인덱스별 단어목록\n",
    "documents, labels = [], []\n",
    "with open('data/SMSSpamCollection') as file_handle:\n",
    "    for line in file_handle: \n",
    "        if line.startswith('spam\\t'):\n",
    "            labels.append(1)\n",
    "            documents.append(line[len('spam\\t'):])\n",
    "        elif line.startswith('ham\\t'):\n",
    "            labels.append(0)\n",
    "            documents.append(line[len('ham\\t'):])\n",
    "\n",
    "# 단어 빈도 피처 (idf 없으면 단어빈도(term frequency)가 생성)\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "documents      = [doc.replace('\\n','').lower() for doc in documents] # 줄바꿈,소문자 변환(차이는 없음)\n",
    "vectorizer     = CountVectorizer()                   # 단어 횟수 피처\n",
    "term_counts    = vectorizer.fit_transform(documents) # 문서내 단어횟수\n",
    "vocabulary     = vectorizer.get_feature_names()      # 인덱스별 단어 List\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(term_counts)\n",
    "features       = tf_transformer.transform(term_counts) # features.todense()\n",
    "features.toarray().shape                             # label, token\n",
    "\n",
    "# import collections\n",
    "# collections.Counter(list(term_counts.toarray()[0])) # Counter Array\n",
    "# collections.Counter(list(features.toarray()[0])) # tf-idf Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) 회귀모델 학습**\n",
    "**sklearn 모듈을** 활용한 **LogisticRegression** 학습 및 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.96842\n",
      "test accuracy : 0.96340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markbaum/Python/python/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2787, 8713)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train, test 50% 분할 뒤 회귀모델을 만들고 검증합니다\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size = 0.5, random_state = 0)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# 검증 데이터 분류허가\n",
    "print(\"train accuracy: {:.5f}\\ntest accuracy : {:.5f}\".format(\n",
    "    classifier.score(X_train, y_train), classifier.score(X_test, y_test)))\n",
    "X_train.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체단어 가중치 목록 갯수:  8713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['score 4.2122 단어: call',\n",
       " 'score 3.5507 단어: txt',\n",
       " 'score 2.9129 단어: free',\n",
       " 'score 2.8318 단어: stop',\n",
       " 'score 2.6789 단어: text']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights : 가중치 값, pairs : 가중치와 text\n",
    "weights, pairs = classifier.coef_[0, :], []\n",
    "for index, value in enumerate(weights):\n",
    "    pairs.append((abs(value), vocabulary[index]))\n",
    "\n",
    "pairs.sort(key=lambda x: x[0], reverse=True) # 분류기 점수정렬\n",
    "print(\"전체단어 가중치 목록 갯수: \", len(pairs))\n",
    "[\"score {:.4f} 단어: {}\".format(par[0], par[1])  for par in pairs[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) 학습된 회귀모델의 활용**\n",
    "훈련당시 데이터와 동일한 환경으로 데이터를 변환하여 예측합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts : again appt is the when yes\n",
      "스팸판단 : [0]\n"
     ]
    }
   ],
   "source": [
    "# 훈련데이터의 일부인 X_train.toarray()[1] 데이터로 스팸여부를 판단\n",
    "text = X_train.toarray()[2]\n",
    "\n",
    "import numpy as np\n",
    "txt = np.argwhere(text!=0).T.tolist()[0]\n",
    "txt = \" \".join(list(map(lambda x : vocabulary[x], txt)))\n",
    "print(\"Texts : {}\\n스팸판단 : {}\".format(\n",
    "    txt, classifier.predict([text])))  # 판단결과 스펨에 해당합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['while the press doesn’t like writing about it',\n",
       " '08006344447 1000 2000 call cash claim freefone gift great guaranteed live news now operator or speak to your',\n",
       " 'nor do i need them to, i donate my yearly presidential salary of $400,000',\n",
       " '00 to different agencies throughout the year, this to homeland security',\n",
       " ' if i didn’t do it there would be hell to pay from the fake news media!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 임의의 Text를 사용하여 스팸여부를 판단\n",
    "\n",
    "sample_txts = \"\"\"While the press doesn’t like writing about it.\n",
    "08006344447 1000 2000 call cash claim freefone gift great guaranteed live news now operator or speak to your.\n",
    "nor do I need them to, I donate my yearly Presidential salary of $400,000.00 to \n",
    "different agencies throughout the year, this to Homeland Security. \n",
    "If I didn’t do it there would be hell to pay from the FAKE NEWS MEDIA!\"\"\"\n",
    "sample_txts = sample_txts.lower()\n",
    "sample_txts = sample_txts.replace('\\n', '')\n",
    "sample_txts = sample_txts.split('.')\n",
    "sample_txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 56)\n"
     ]
    }
   ],
   "source": [
    "# feacture 변환\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "vectorizer     = CountVectorizer()                     # 단어 횟수 피처\n",
    "term_counts    = vectorizer.fit_transform(sample_txts) # 문서내 단어횟수\n",
    "voca           = vectorizer.get_feature_names()        # 인덱스별 단어 List\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(term_counts)\n",
    "features       = tf_transformer.transform(term_counts) # features.todense()\n",
    "print(features.toarray().shape)                        # label, token               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_text = []\n",
    "for sentence in features.toarray():\n",
    "    sent_arr = [0] * len(vocabulary)     # 빈 array 생성\n",
    "    for no, tf_t in enumerate(sentence): # 단어별 빈도값 추출\n",
    "        if tf_t != 0:                    # 유효값이 발견시\n",
    "            if voca[no] in vocabulary:   # 학습가능 단어 포함여부 확인\n",
    "                sent_arr[vocabulary.index(voca[no])] = sentence[no]\n",
    "    array_text.append(sent_arr)\n",
    "array_text = np.array(array_text)\n",
    "\n",
    "classifier.predict(array_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **02 LDA 모델을 활용한 주제어 분석**\n",
    "- 위에서 추출한 스팸데이터를 활용하여 **LDA 알고리즘으로** 주제별 단어를 묶습니다\n",
    "- 전형적인 **비지도 학습** 알고리즘 입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=PendingDeprecationWarning)\n",
    "vectorizer  = CountVectorizer(stop_words='english', max_features=2000)\n",
    "term_counts = vectorizer.fit_transform(documents)\n",
    "vocabulary  = vectorizer.get_feature_names()\n",
    "            \n",
    "# LDA는 단어출현 갯수로 동작하므로 CountVectorizer를 활용\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "topic_model = LatentDirichletAllocation(n_components=10)\n",
    "topic_model.fit(term_counts) # 토픽 모델을 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0: ll,later,sorry,hi,think,know,im,need,yeah,gonna,\n",
      "topic 1: don,home,know,just,come,buy,going,time,want,stuff,\n",
      "topic 2: good,day,love,dear,hope,happy,night,just,like,want,\n",
      "topic 3: ok,lor,like,wat,ask,dun,thk,ur,yup,home,\n",
      "topic 4: ur,going,tomorrow,just,got,work,ll,class,doing,come,\n",
      "topic 5: tell,ur,week,dont,just,life,oh,sorry,tone,txt,\n",
      "topic 6: stop,txt,text,com,ur,www,new,win,service,msg,\n",
      "topic 7: got,da,send,ur,pick,cash,right,message,phone,way,\n",
      "topic 8: reply,text,just,free,yes,stop,mobile,live,claim,ringtone,\n",
      "topic 9: gt,lt,free,prize,mobile,won,claim,urgent,send,guaranteed,\n"
     ]
    }
   ],
   "source": [
    "# 학습된 토픽들을 하나씩 출력합니다.\n",
    "topics = topic_model.components_\n",
    "for topic_id, weights in enumerate(topics):\n",
    "    print('topic %d' % topic_id, end=': ')\n",
    "    pairs = [(abs(value), vocabulary[term_id]) for term_id, value in enumerate(weights)]\n",
    "    pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "    for pair in pairs[:10]: print(pair[1], end=',')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "# **2 내용기반 추천 알고리즘**\n",
    "- [**(GitHub)**](https://github.com/your-first-ml-book/Examples) **처음 배우는 머신러닝** 12장 영화 추천 시스템 만들기\n",
    "- **movie_info_li : 영화의 정보**, **movie_plot_li : 영화의 줄거리** 저장\n",
    "```\n",
    "original_title \toverview\n",
    "0 \tToy Story \tLed by Woody, Andy's toys live happily in his ...\n",
    "1 \tJumanji \tWhen siblings Judy and Peter discover an encha... ```\n",
    "\n",
    "## **01 영화 자료 불러오기**\n",
    "영화 제목과, 영화 줄거리 정보를 불러옵니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44506, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toy Story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>Toy Story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>Jumanji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     original_title                                           overview  \\\n",
       "0         Toy Story  Led by Woody, Andy's toys live happily in his ...   \n",
       "1           Jumanji  When siblings Judy and Peter discover an encha...   \n",
       "2  Grumpier Old Men  A family wedding reignites the ancient feud be...   \n",
       "\n",
       "              title  \n",
       "0         Toy Story  \n",
       "1           Jumanji  \n",
       "2  Grumpier Old Men  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "movies = pd.read_csv('data/movies_metadata.csv', usecols=['original_title', 'overview', 'title'], low_memory=False)\n",
    "movies = movies.dropna(axis=0)\n",
    "print(movies.shape)\n",
    "\n",
    "movie_plot_li = movies['overview']\n",
    "movie_info_li = movies['title']\n",
    "movies.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['original_title', 'overview', 'title'], dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **02 영화 줄거리를 활용한 Cosin 유사도 모델**\n",
    "- **[A-z]** 정규식 과 **WordNetLemmatizer** 을 활용하여 분석가능한 객체를 추출\n",
    "- **Memory Error** 로 인해 전체가 아닌 일부만 학습을 진행합니다 \n",
    "- 다른 PC에서 학습 후 **pickle** 로 적용, 활용하는 방법도 가능합니다\n",
    "\n",
    "### **1) cosin 유사도를 사용한 모델링**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markbaum/Python/python/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10000)\n",
      "CPU times: user 5.13 s, sys: 545 ms, total: 5.67 s\n",
      "Wall time: 5.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 잘 전처리된 token을 추출 합니다\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl       = WordNetLemmatizer()\n",
    "        self.tokenizer = RegexpTokenizer('(?u)[A-z]+')\n",
    "    \n",
    "    def __call__(self, doc):  # 클래스 호출시 마다 실행(Tf-idf Vector 호출)\n",
    "        return([self.wnl.lemmatize(t) for t in self.tokenizer.tokenize(doc)])\n",
    "\n",
    "# 사이킷런에 위에서 정의한 토크나이저를 입력으로 넣습니다.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer   = TfidfVectorizer(min_df=3, tokenizer=LemmaTokenizer(), stop_words='english')\n",
    "X            = vectorizer.fit_transform(movie_plot_li[:10000]) # 메모리 오류로 갯수를 제한\n",
    "vocabluary   = vectorizer.get_feature_names()\n",
    "\n",
    "# 비슷한 영화 추천하는 Cosin 유사모델 만들기\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "movie_sim = cosine_similarity(X)\n",
    "print(movie_sim.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) 학습 모델을 활용한 유사측정 영화목록 출력하기**\n",
    "- **movie_sim** 유사도 측정 Matrix 를 사용하여, 인덱스별 유사도를 측정 가능합니다\n",
    "- 위의 추출내용을 **reverse=True** 로 데이터를 정렬 한 뒤, 상위 객체들을 출력합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Georgia : 관람객 추천영화 -------\n",
      "추천영화 0순위 : Don't Look in the Basement\n",
      "추천영화 1순위 : Sour Grapes\n",
      "추천영화 2순위 : James and the Giant Peach\n",
      "추천영화 3순위 : White Zombie\n",
      "추천영화 4순위 : The Man Who Knew Too Little\n",
      "추천영화 5순위 : Heaven's Burning\n",
      "추천영화 6순위 : Made\n"
     ]
    }
   ],
   "source": [
    "# 특정 영화와 유사한 영화목록 출력하기\n",
    "def similar_recommend_by_movie_id(movielens_id, rank=8):\n",
    "    movie_index    = movielens_id - 1\n",
    "    similar_movies = sorted(list(enumerate(movie_sim[movie_index])),key=lambda x:x[1], reverse=True)\n",
    "    # 유사도 측정결과를 출력\n",
    "    print(\"----- {} : 관람객 추천영화 -------\".format(movie_info_li[similar_movies[0][0]]))\n",
    "    for no, movie_idx in enumerate(similar_movies[1:rank]):\n",
    "        print('추천영화 %d순위 : %s'%(no, movie_info_li[movie_idx[0]]))\n",
    "        \n",
    "similar_recommend_by_movie_id(55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **03 영화 줄거리를 활용한 linear_kernel 유사도 모델**\n",
    "위에서 구현되는 기능을 보다 간결하게 처리하는 방법을 구현합니다\n",
    "- 도서 : **머신러닝 완벽 가이드 (2019/3)** [**GitHub**](https://github.com/wikibook/ml-definitive-guide)\n",
    "- 개념들 보완 : 딥러닝 활용한 자연어 입문 [**(WikiBook)**](https://wikidocs.net/24603)\n",
    "\n",
    "### **1) linear_kernel 유사도를 사용한 모델링**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markbaum/Python/python/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 35540)\n"
     ]
    }
   ],
   "source": [
    "# 5,000 개의 영화, 22,304 단어행렬 (stopword 제거)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "data             = movies.head(12000)             # 부하를 줄이기 위해 5000개만 추출\n",
    "tfidf            = TfidfVectorizer(stop_words='english')\n",
    "data['overview'] = data['overview'].fillna('') # 줄거리 NaN 면 인덱스 제거\n",
    "tfidf_matrix     = tfidf.fit_transform(data['overview'])\n",
    "print(tfidf_matrix.shape)                      # overview에 대해서 tf-idf 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Father of the Bride Part II 의 인덱스:  4\n",
      "CPU times: user 815 ms, sys: 653 ms, total: 1.47 s\n",
      "Wall time: 1.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TF-IDF Vectorizer간 Dot Product 계산 Score 제공\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "indices    = pd.Series(data.index, index=data['title']).drop_duplicates()\n",
    "idx        = indices['Father of the Bride Part II']\n",
    "print(\"\\nFather of the Bride Part II 의 인덱스: \", idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) 학습한 모델을 활용하기**\n",
    "linear_kernel 유사도 모델을 사용하여 결과를 출력합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     Toy Story\n",
       "2997                Toy Story 2\n",
       "10301    The 40 Year Old Virgin\n",
       "8327                  The Champ\n",
       "1071      Rebel Without a Cause\n",
       "11399    For Your Consideration\n",
       "1932                  Condorman\n",
       "3057            Man on the Moon\n",
       "485                      Malice\n",
       "11606              Factory Girl\n",
       "5797              Class of 1984\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OverView 데이터를 사용하여 영화간 유사도를 측정합니다\n",
    "def get_recommendations(idx, cosine_sim=cosine_sim, rank=11):\n",
    "#     idx        = indices[title]                # 해당영화의 타이틀로 인덱스를 호출\n",
    "    sim_scores = list(enumerate(cosine_sim[idx])) # 모든 영화에 대한 해당영화 유사도\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True) # 유사도 정렬\n",
    "    sim_scores = sim_scores[0: rank]           # 가장 유사한 10개의 영화를 받아옵니다.\n",
    "    movie_indices = [i[0] for i in sim_scores] # 유사도 높은 10개 영화\n",
    "    return data['title'].iloc[movie_indices]   # 가장 유사한 10개의 영화의 제목을 리턴\n",
    "\n",
    "get_recommendations(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "# **3 협업 필터링**\n",
    "- **협업 필터링을** 활용하면 **보다 적은 데이터로** 학습이 가능합니다\n",
    "- **surprise, fastFM** 등의 모듈을 활용하면 보다 쉬운 접근이 가능합니다\n",
    "\n",
    "## **01 fastFM 모듈을 사용한 예측 모델링**\n",
    "- **surprise** 모델보다 **다양한 데이터 행렬을** 사용한 학습이 가능합니다\n",
    "- **학습용 dataset과, label 데이터셋을** 구분한 뒤 **모델을 학습** 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'item=10', 'item=20', 'item=43', 'item=5', 'user=1', 'user=2', 'user=3', 'user=4']\n",
      "[[19.  0.  0.  0.  1.  1.  0.  0.  0.]\n",
      " [33.  0.  0.  1.  0.  0.  1.  0.  0.]\n",
      " [55.  0.  1.  0.  0.  0.  0.  1.  0.]\n",
      " [20.  1.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# 나이를 제외한 정보는 범주형으로 변환됩니다\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "train = [\n",
    "    {\"user\": \"1\", \"item\": \"5\", \"age\": 19},\n",
    "    {\"user\": \"2\", \"item\": \"43\", \"age\": 33},\n",
    "    {\"user\": \"3\", \"item\": \"20\", \"age\": 55},\n",
    "    {\"user\": \"4\", \"item\": \"10\", \"age\": 20},\n",
    "]\n",
    "v = DictVectorizer()       # Key 피쳐값은 v.feature_names_ 저장\n",
    "X = v.fit_transform(train) # age값, user BOW(4), item BOW(4)\n",
    "print(v.feature_names_)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FMRegression(init_stdev=0.1, l2_reg=0, l2_reg_V=0.5, l2_reg_w=0.1,\n",
       "       n_iter=1000, random_state=123, rank=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1~4 user 들의 평점(Target) 정보를 활용하여 회귀모델을 학습\n",
    "import numpy as np\n",
    "y  = np.array([5.0, 1.0, 2.0, 4.0]) \n",
    "\n",
    "from fastFM import als\n",
    "fm = als.FMRegression(n_iter = 1000, \n",
    "                      init_stdev = 0.1, \n",
    "                      rank = 2, \n",
    "                      l2_reg_w = 0.1, \n",
    "                      l2_reg_V = 0.5)\n",
    "fm.fit(X, y)\n",
    "fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.60775939])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 새로운 사용자 정보를 입력하면 예측 3.6 평점을 출력 합니다\n",
    "fm.predict(v.transform({\"user\": \"5\", \"item\": \"10\", \"age\": 24}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **02 Surprise 모듈을 사용한 예측 모델링**\n",
    "**협업 필터링은 itemID, userID, rating** 3개의 필드 값 정보로만 구성 됩니다\n",
    "\n",
    "### **1) 교차 검증(Cross Validation)과 하이퍼 파라미터 튜닝**\n",
    "- **K-Fold** 교차검증을 사용하여 **검증 알고리즘** 을 확인 합니다\n",
    "- **GridSearchCV** 등을 사용하여 **최적의 파라미터를** 찾습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.8987  0.8956  0.8979  0.8953  0.8945  0.8964  0.0016  \n",
      "MAE (testset)     0.6933  0.6914  0.6887  0.6880  0.6877  0.6898  0.0022  \n",
      "Fit time          3.92    3.98    3.93    3.93    4.37    4.03    0.17    \n",
      "Test time         0.14    0.13    0.13    0.21    0.13    0.15    0.03    \n",
      "CPU times: user 21.8 s, sys: 76.9 ms, total: 21.9 s\n",
      "Wall time: 22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from surprise import Reader, Dataset, SVD\n",
    "from surprise.model_selection import cross_validate \n",
    "ratings = pd.read_csv('./data/ml-latest-small/ratings.csv')\n",
    "reader  = Reader(rating_scale = (0.5, 5.0))  # Reader 인스턴스\n",
    "data    = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
    "algo    = SVD(random_state = 0) \n",
    "cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bast Score: 0.899165305444369\n",
      "Params: {'n_epochs': 20, 'n_factors': 50}\n",
      "CPU times: user 1min 5s, sys: 540 ms, total: 1min 6s\n",
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 최적화 검증용 파라미터, GridSearchCV (KFold CV 3개 분할)\n",
    "from surprise.model_selection import GridSearchCV\n",
    "param_grid = {'n_epochs': [20, 40, 60], 'n_factors': [50, 100, 200] }\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3, n_jobs=-1)\n",
    "gs.fit(data)\n",
    "\n",
    "# 최고 RMSE Evaluation 점수와 그때의 하이퍼 파라미터\n",
    "print(\"Bast Score: {}\\nParams: {}\".format(\n",
    "    gs.best_score['rmse'], gs.best_params['rmse']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) 최적의 파라미터를 사용한 SVD 회귀모델 학습**\n",
    "- 위에서 사용한 **K-Fold** 와 **CrossValidation** 결과를 활용합니다\n",
    "- **sklearn 모듈을** 활용한 **LogisticRegression** 학습 및 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.54 s, sys: 15.9 ms, total: 2.56 s\n",
      "Wall time: 2.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "from surprise import Reader, Dataset, SVD\n",
    "# ratings = pd.read_csv('./data/ml-latest-small/ratings.csv') \n",
    "# ratings.head(3)\n",
    "\n",
    "# UserID, MovieID, ratting 3개 필드만 학습\n",
    "# reader  = Reader(rating_scale=(0.5, 5.0))\n",
    "# data    = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
    "algo    = SVD(n_factors=50, n_epochs=20, random_state=0)\n",
    "\n",
    "from surprise.model_selection import train_test_split\n",
    "trainset, testset = train_test_split(data, test_size=.25, random_state=0)\n",
    "algo.fit(trainset)\n",
    "algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(30, 254, 3.7092901755922996),\n",
       " (652, 2626, 4.172573234363497),\n",
       " (466, 2174, 3.673029601193435)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = algo.test(testset)\n",
    "[(pred.uid, pred.iid, pred.est)  for pred in predictions[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8908\n",
      "0.8907754769926038\n",
      "user: 196        item: 302        r_ui = None   est = 3.54   {'was_impossible': False}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('예측평점 :', 3.541978320867165)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import accuracy \n",
    "print(accuracy.rmse(predictions))\n",
    "\n",
    "# 특정한 사용자/ 영화의 유사도 측정\n",
    "userid, movieid  = str(196), str(302)\n",
    "pred = algo.predict(userid, movieid)\n",
    "print(pred)\n",
    "\"예측평점 :\", pred.est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **03 잠재요인 협업 필터링**\n",
    "### **1) 경사 하강법을 활용한 행렬분해**\n",
    "- **user/ item based table** 을 예측하는데 있어서 $U, V$ 2개의 압축합니다\n",
    "- 그리고 $U, V$ **행렬의 곱을** 활용하여 **비어있는 값들을 예측하는 테이블을** 생성합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 행렬 R 생성, 분해 행렬 P와 Q 초기화\n",
    "# K-Fold 교차 잠재요인의 K 를 3 으로 설정 \n",
    "import numpy as np\n",
    "R = np.array([[4, np.NaN, np.NaN, 2, np.NaN ],\n",
    "              [np.NaN, 5, np.NaN, 3, 1 ],\n",
    "              [np.NaN, np.NaN, 3, 4, 4 ],\n",
    "              [5, 2, 1, 2, np.NaN ]])\n",
    "\n",
    "# P와 Q 매트릭스를 지정하고 정규분포의 random 값으로 채웁니다\n",
    "num_users, num_items = R.shape\n",
    "K = 3\n",
    "np.random.seed(1234)\n",
    "P = np.random.normal(scale=1./K, size=(num_users, K))\n",
    "Q = np.random.normal(scale=1./K, size=(num_items, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def get_rmse(R, P, Q, non_zeros):\n",
    "    error = 0\n",
    "    full_pred_matrix = np.dot(P, Q.T)  # 행렬 P와 Q.T의 내적으로 예측 R 생성    \n",
    "    # 실제 R 행렬에서 널이 아닌 값의 위치 인덱스 추출하여 실제 R 행렬과 예측 행렬의 RMSE 추출\n",
    "    x_non_zero_ind = [non_zero[0] for non_zero in non_zeros]\n",
    "    y_non_zero_ind = [non_zero[1] for non_zero in non_zeros]\n",
    "    R_non_zeros    = R[x_non_zero_ind, y_non_zero_ind]\n",
    "    full_pred_matrix_non_zeros = full_pred_matrix[x_non_zero_ind, y_non_zero_ind]\n",
    "    mse  = mean_squared_error(R_non_zeros, full_pred_matrix_non_zeros)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 반복학습 Step :     0 RMSE : 3.2843\n",
      "# 반복학습 Step :   200 RMSE : 0.0218\n",
      "# 반복학습 Step :   400 RMSE : 0.0146\n",
      "# 반복학습 Step :   600 RMSE : 0.0144\n",
      "# 반복학습 Step :   800 RMSE : 0.0144\n",
      "# 반복학습 Step : 1,000 RMSE : 0.0144\n"
     ]
    }
   ],
   "source": [
    "# R > 0 인 행 위치, 열 위치, 값을 non_zeros 리스트에 저장. \n",
    "non_zeros = [(i, j, R[i,j])  for i in range(num_users) \n",
    "                             for j in range(num_items) \n",
    "                             if R[i,j] > 0 ]\n",
    "steps, learning_rate, r_lambda = 1001, 0.01, 0.01\n",
    "\n",
    "# SGD 기법으로 P와 Q 매트릭스를 계속 업데이트. \n",
    "for step in range(steps):\n",
    "    for i, j, r in non_zeros:\n",
    "        # Regularization을 반영한 SGD 업데이트 공식\n",
    "        eij    = r - np.dot(P[i, :], Q[j, :].T) # 실제 값과 예측 값의 차이\n",
    "        P[i,:] = P[i,:] + learning_rate*(eij * Q[j, :] - r_lambda*P[i,:])\n",
    "        Q[j,:] = Q[j,:] + learning_rate*(eij * P[i, :] - r_lambda*Q[j,:])\n",
    "\n",
    "    rmse = get_rmse(R, P, Q, non_zeros)\n",
    "    if (step % 200) == 0 :\n",
    "        print(\"# 반복학습 Step : {:5,} RMSE : {:.4f}\".format(step, rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습전 행렬: \n",
      "[[ 4. nan nan  2. nan]\n",
      " [nan  5. nan  3.  1.]\n",
      " [nan nan  3.  4.  4.]\n",
      " [ 5.  2.  1.  2. nan]]\n",
      "\n",
      "경사하강 예측 행렬: \n",
      "[[3.992 1.68  1.197 1.998 1.458]\n",
      " [5.41  4.976 0.659 2.987 1.005]\n",
      " [5.296 2.325 2.988 3.98  3.985]\n",
      " [4.971 2.005 1.004 2.004 1.095]]\n"
     ]
    }
   ],
   "source": [
    "pred_matrix = np.dot(P, Q.T)\n",
    "print('학습전 행렬: \\n{}\\n\\n경사하강 예측 행렬: \\n{}'.format(\n",
    "    R, np.round(pred_matrix, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) 영화 리뷰 데이터를 활용한 행렬분해**\n",
    "리뷰를 사용한 예측행렬을 생성합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# RMSE 결과예측 함수를 정의합니다\n",
    "def get_rmse(R, P, Q, non_zeros):\n",
    "    error = 0\n",
    "    full_pred_matrix = np.dot(P, Q.T) # P와 Q.T의 내적 곱 : 예측 R 행렬\n",
    "    # 실제 R 행렬에서 NaN 아닌 값만 추출하여 실제 R 행렬과 예측 행렬의 RMSE 추출\n",
    "    x_non_zero_ind = [non_zero[0] for non_zero in non_zeros]\n",
    "    y_non_zero_ind = [non_zero[1] for non_zero in non_zeros]\n",
    "    R_non_zeros    = R[x_non_zero_ind, y_non_zero_ind]\n",
    "    full_pred_matrix_non_zeros = full_pred_matrix[x_non_zero_ind, y_non_zero_ind]\n",
    "    mse  = mean_squared_error(R_non_zeros, full_pred_matrix_non_zeros)\n",
    "    rmse = np.sqrt(mse)    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행렬 분해 후, 압축행렬의 곱으로 결과를 생성합니다\n",
    "def matrix_factorization(R, K, steps=200, learning_rate=0.01, r_lambda = 0.01):\n",
    "    num_users, num_items = R.shape\n",
    "    # P와 Q 매트릭스의 크기를 지정하고 정규분포를 가진 랜덤한 값으로 입력합니다. \n",
    "    np.random.seed(1)\n",
    "    P = np.random.normal(scale=1./K, size=(num_users, K))\n",
    "    Q = np.random.normal(scale=1./K, size=(num_items, K))\n",
    "    break_count = 0\n",
    "    # R > 0 인 행 위치, 열 위치, 값을 non_zeros 리스트 객체에 저장. \n",
    "    non_zeros = [ (i, j, R[i,j]) for i in range(num_users) for j in range(num_items) if R[i,j] > 0 ] \n",
    "    for step in range(steps):      # SGD기법으로 P와 Q 매트릭스를 계속 업데이트\n",
    "        for i, j, r in non_zeros:  # 실제 값과 예측 값의 차이인 오류 값 구함\n",
    "            eij = r - np.dot(P[i, :], Q[j, :].T)\n",
    "            # Regularization을 반영한 SGD 업데이트 공식 적용\n",
    "            P[i,:] = P[i,:] + learning_rate*(eij * Q[j, :] - r_lambda*P[i,:])\n",
    "            Q[j,:] = Q[j,:] + learning_rate*(eij * P[i, :] - r_lambda*Q[j,:])\n",
    "        rmse = get_rmse(R, P, Q, non_zeros)\n",
    "        if (step % 20) == 0 :\n",
    "            print(\"# 학습의 반복: {:3} rmse: {:.4f}\".format(step, rmse))\n",
    "    return P, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 학습의 반복:   0 rmse: 2.9321\n",
      "# 학습의 반복:  20 rmse: 0.5275\n",
      "# 학습의 반복:  40 rmse: 0.3017\n",
      "# 학습의 반복:  60 rmse: 0.2261\n",
      "# 학습의 반복:  80 rmse: 0.1943\n",
      "CPU times: user 2min 43s, sys: 21.7 s, total: 3min 5s\n",
      "Wall time: 2min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "movies  = pd.read_csv('./data/ml-latest-small/movies.csv')\n",
    "ratings = pd.read_csv('./data/ml-latest-small/ratings.csv')\n",
    "ratings = ratings[['userId', 'movieId', 'rating']]\n",
    "# columns='title' 로 title 컬럼으로 pivot 수행. \n",
    "ratings_matrix = ratings.pivot_table('rating', index='userId', columns='movieId')\n",
    "rating_movies  = pd.merge(ratings, movies, on='movieId') # title 컬럼을 movies 와 Join\n",
    "ratings_matrix = rating_movies.pivot_table('rating', index='userId', columns='title')\n",
    "\n",
    "# 압축행렬을 생성한뒤, 행렬곱을 사용하여 비어있는 값 들을 채웁니다\n",
    "P, Q = matrix_factorization(ratings_matrix.values, K=50, steps=100, learning_rate=0.01, r_lambda = 0.01)\n",
    "pred_matrix = np.dot(P, Q.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>title</th>\n",
       "      <th>\"Great Performances\" Cats (1998)</th>\n",
       "      <th>$9.99 (2008)</th>\n",
       "      <th>'Hellboy': The Seeds of Creation (2004)</th>\n",
       "      <th>'Neath the Arizona Skies (1934)</th>\n",
       "      <th>'Round Midnight (1986)</th>\n",
       "      <th>'Salem's Lot (2004)</th>\n",
       "      <th>'Til There Was You (1997)</th>\n",
       "      <th>'burbs, The (1989)</th>\n",
       "      <th>'night Mother (1986)</th>\n",
       "      <th>(500) Days of Summer (2009)</th>\n",
       "      <th>...</th>\n",
       "      <th>Zulu (1964)</th>\n",
       "      <th>Zulu (2013)</th>\n",
       "      <th>[REC] (2007)</th>\n",
       "      <th>eXistenZ (1999)</th>\n",
       "      <th>loudQUIETloud: A Film About the Pixies (2006)</th>\n",
       "      <th>xXx (2002)</th>\n",
       "      <th>xXx: State of the Union (2005)</th>\n",
       "      <th>¡Three Amigos! (1986)</th>\n",
       "      <th>À nous la liberté (Freedom for Us) (1931)</th>\n",
       "      <th>İtirazım Var (2014)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.745341</td>\n",
       "      <td>2.928824</td>\n",
       "      <td>1.150022</td>\n",
       "      <td>0.317918</td>\n",
       "      <td>1.286991</td>\n",
       "      <td>1.871280</td>\n",
       "      <td>1.846075</td>\n",
       "      <td>2.646222</td>\n",
       "      <td>2.902585</td>\n",
       "      <td>2.661586</td>\n",
       "      <td>...</td>\n",
       "      <td>2.466481</td>\n",
       "      <td>0.665347</td>\n",
       "      <td>2.536329</td>\n",
       "      <td>1.512936</td>\n",
       "      <td>2.512019</td>\n",
       "      <td>2.210895</td>\n",
       "      <td>0.533123</td>\n",
       "      <td>2.700853</td>\n",
       "      <td>2.361938</td>\n",
       "      <td>2.139596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.162452</td>\n",
       "      <td>3.686570</td>\n",
       "      <td>1.542740</td>\n",
       "      <td>0.318964</td>\n",
       "      <td>1.740465</td>\n",
       "      <td>2.584770</td>\n",
       "      <td>2.545283</td>\n",
       "      <td>3.196354</td>\n",
       "      <td>3.670584</td>\n",
       "      <td>2.873576</td>\n",
       "      <td>...</td>\n",
       "      <td>3.374175</td>\n",
       "      <td>0.777538</td>\n",
       "      <td>2.977572</td>\n",
       "      <td>1.327954</td>\n",
       "      <td>3.101721</td>\n",
       "      <td>3.230171</td>\n",
       "      <td>0.632209</td>\n",
       "      <td>2.661355</td>\n",
       "      <td>2.884804</td>\n",
       "      <td>2.845496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.250018</td>\n",
       "      <td>3.652691</td>\n",
       "      <td>1.543458</td>\n",
       "      <td>0.380940</td>\n",
       "      <td>1.689739</td>\n",
       "      <td>2.633695</td>\n",
       "      <td>2.563878</td>\n",
       "      <td>3.315305</td>\n",
       "      <td>4.030388</td>\n",
       "      <td>3.184506</td>\n",
       "      <td>...</td>\n",
       "      <td>3.468197</td>\n",
       "      <td>1.087605</td>\n",
       "      <td>3.537009</td>\n",
       "      <td>3.230556</td>\n",
       "      <td>3.345892</td>\n",
       "      <td>1.413009</td>\n",
       "      <td>0.627127</td>\n",
       "      <td>2.872369</td>\n",
       "      <td>3.217418</td>\n",
       "      <td>2.857210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 9064 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "title   \"Great Performances\" Cats (1998)  $9.99 (2008)  \\\n",
       "userId                                                   \n",
       "1                               1.745341      2.928824   \n",
       "2                               2.162452      3.686570   \n",
       "3                               2.250018      3.652691   \n",
       "\n",
       "title   'Hellboy': The Seeds of Creation (2004)  \\\n",
       "userId                                            \n",
       "1                                      1.150022   \n",
       "2                                      1.542740   \n",
       "3                                      1.543458   \n",
       "\n",
       "title   'Neath the Arizona Skies (1934)  'Round Midnight (1986)  \\\n",
       "userId                                                            \n",
       "1                              0.317918                1.286991   \n",
       "2                              0.318964                1.740465   \n",
       "3                              0.380940                1.689739   \n",
       "\n",
       "title   'Salem's Lot (2004)  'Til There Was You (1997)  'burbs, The (1989)  \\\n",
       "userId                                                                       \n",
       "1                  1.871280                   1.846075            2.646222   \n",
       "2                  2.584770                   2.545283            3.196354   \n",
       "3                  2.633695                   2.563878            3.315305   \n",
       "\n",
       "title   'night Mother (1986)  (500) Days of Summer (2009)  \\\n",
       "userId                                                      \n",
       "1                   2.902585                     2.661586   \n",
       "2                   3.670584                     2.873576   \n",
       "3                   4.030388                     3.184506   \n",
       "\n",
       "title          ...           Zulu (1964)  Zulu (2013)  [REC] (2007)  \\\n",
       "userId         ...                                                    \n",
       "1              ...              2.466481     0.665347      2.536329   \n",
       "2              ...              3.374175     0.777538      2.977572   \n",
       "3              ...              3.468197     1.087605      3.537009   \n",
       "\n",
       "title   eXistenZ (1999)  loudQUIETloud: A Film About the Pixies (2006)  \\\n",
       "userId                                                                   \n",
       "1              1.512936                                       2.512019   \n",
       "2              1.327954                                       3.101721   \n",
       "3              3.230556                                       3.345892   \n",
       "\n",
       "title   xXx (2002)  xXx: State of the Union (2005)  ¡Three Amigos! (1986)  \\\n",
       "userId                                                                      \n",
       "1         2.210895                        0.533123               2.700853   \n",
       "2         3.230171                        0.632209               2.661355   \n",
       "3         1.413009                        0.627127               2.872369   \n",
       "\n",
       "title   À nous la liberté (Freedom for Us) (1931)  İtirazım Var (2014)  \n",
       "userId                                                                  \n",
       "1                                        2.361938             2.139596  \n",
       "2                                        2.884804             2.845496  \n",
       "3                                        3.217418             2.857210  \n",
       "\n",
       "[3 rows x 9064 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습된 행렬을 사용하여 Pivot Table을 출력합니다\n",
    "ratings_pred_matrix = pd.DataFrame(data = pred_matrix, \n",
    "                                   index = ratings_matrix.index,\n",
    "                                   columns = ratings_matrix.columns)\n",
    "ratings_pred_matrix.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
